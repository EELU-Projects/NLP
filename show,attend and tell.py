# -*- coding: utf-8 -*-
"""Untitled7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AZQFpwsJgUZtV8Gt5wZoE3Sfvu_iFSVE
"""

from google.colab import files
files.upload()

!pip install -q kaggle  # تنزيل Kaggle API
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json  #

!kaggle datasets download -d adityajn105/flickr8k
!unzip flickr8k.zip -d data

!ls data  # يجب أن تظهر المجلدات مثل "Images" و"Text

import pandas as pd
import os

# تحميل ملف الوصف
captions = pd.read_csv("data/captions.txt")
print(captions.head())

# عرض مثال لصورة + وصفها
from IPython.display import Image, display
sample_image = os.path.join("data/Images", captions.iloc[0]["image"])
display(Image(sample_image))
print("Caption:", captions.iloc[0]["caption"])

import pandas as pd
import numpy as np
from PIL import Image

# تحميل ملف الوصف
captions = pd.read_csv("data/captions.txt")
captions['caption'] = captions['caption'].apply(lambda x: x.lower().strip())
print(captions.head())

# مثال لعرض صورة مع وصفها
from IPython.display import display
sample = captions.iloc[0]
img_path = f"data/Images/{sample['image']}"
display(Image.open(img_path))
print("Caption:", sample['caption'])



import os
import pandas as pd
from PIL import Image
import numpy as np

# تحميل بيانات التوصيف
captions = pd.read_csv("data/captions.txt")
captions['caption'] = captions['caption'].apply(lambda x: x.lower().strip())

# اختيار 1000 صورة فقط
selected_images = captions.iloc[:1000]

# التأكد من توفر الصور
image_paths = ["data/Images/" + img for img in selected_images['image'] if os.path.exists("data/Images/" + img)]
captions = selected_images[selected_images['image'].isin([os.path.basename(img) for img in image_paths])]

print("عدد الصور المتاحة:", len(image_paths))

import tensorflow as tf
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
from tensorflow.keras.models import Sequential

# بناء نموذج CNN لاستخراج الميزات
cnn_model = Sequential([
    Conv2D(32, (3,3), activation='relu', input_shape=(224, 224, 3)),
    MaxPooling2D(2,2),
    Conv2D(64, (3,3), activation='relu'),
    MaxPooling2D(2,2),
    Flatten(),
    Dense(256, activation='relu'),
    Dense(128, activation='relu')
])

# استخراج الميزات من الصور
def extract_features(img_path):
    img = Image.open(img_path).resize((224, 224))
    img_array = tf.keras.preprocessing.image.img_to_array(img) / 255.0
    img_array = tf.expand_dims(img_array, axis=0)
    features = cnn_model(img_array)
    return features.numpy()

# استخراج الميزات لجميع الصور
image_features = {img: extract_features(img) for img in image_paths}
print("تم استخراج الميزات!")

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# إعداد tokenizer
tokenizer = Tokenizer(num_words=5000, oov_token="<OOV>")
tokenizer.fit_on_texts(captions["caption"])

# تحويل التوصيفات إلى أرقام
sequences = tokenizer.texts_to_sequences(captions["caption"])
padded_sequences = pad_sequences(sequences, maxlen=30, padding="post")

# التأكد من أن الصور لها ميزات مستخرجة
valid_images = [img for img in captions["image"] if img in image_features]
captions = captions[captions["image"].isin(valid_images)]

# تحويل الميزات من dictionary إلى مصفوفة
image_feature_array = np.array([image_features[img] for img in valid_images])

# التأكد من تطابق حجم البيانات بعد التصحيح
assert image_feature_array.shape[0] == len(captions), "❌ عدد الصور لا يتطابق مع التوصيفات!"

print("✅ عدد الصور بعد التصحيح:", image_feature_array.shape[0])
print("✅ عدد التوصيفات بعد التصحيح:", len(padded_sequences))

# تقسيم البيانات إلى تدريب واختبار
split_ratio = 0.8
split_index = int(len(padded_sequences) * split_ratio)

X_train = image_feature_array[:split_index]
y_train = padded_sequences[:split_index]

X_val = image_feature_array[split_index:]
y_val = padded_sequences[split_index:]

print("✅ بيانات التدريب جاهزة:", X_train.shape, y_train.shape)
print("✅ بيانات التحقق جاهزة:", X_val.shape, y_val.shape)

import tensorflow.keras.layers as layers
from tensorflow.keras.models import Sequential

# إعداد نموذج LSTM لتوليد الوصف
vocab_size = 5000
embedding_dim = 256
max_length = 30

language_model = Sequential([
    layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length),
    layers.LSTM(256, return_sequences=True),
    layers.LSTM(256),
    layers.Dense(vocab_size, activation="softmax")
])

language_model.summary()

import tensorflow.keras.backend as K

class AttentionLayer(layers.Layer):
    def __init__(self):
        super(AttentionLayer, self).__init__()

    def call(self, hidden_states):
        score = layers.Dense(1, activation="tanh")(hidden_states)
        attention_weights = tf.nn.softmax(score, axis=1)
        context_vector = attention_weights * hidden_states
        return K.sum(context_vector, axis=1)

# دمج الطبقة داخل النموذج
inputs = layers.Input(shape=(max_length,))
embeddings = layers.Embedding(vocab_size, embedding_dim)(inputs)
lstm_output = layers.LSTM(256, return_sequences=True)(embeddings)
context_vector = AttentionLayer()(lstm_output)
output = layers.Dense(vocab_size, activation="softmax")(context_vector)

attention_model = tf.keras.models.Model(inputs, output)
attention_model.summary()

from tensorflow.keras.callbacks import EarlyStopping

early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

language_model.compile(optimizer="adam", loss="sparse_categorical_crossentropy", metrics=["accuracy"])
history_lstm = language_model.fit(X_train, y_train, validation_data=(X_val, y_val),
                                  epochs=50, batch_size=32, callbacks=[early_stopping])

attention_model.compile(optimizer="adam", loss="sparse_categorical_crossentropy", metrics=["accuracy"])
history_attention = attention_model.fit(X_train, y_train, validation_data=(X_val, y_val),
                                        epochs=50, batch_size=32, callbacks=[early_stopping])

import matplotlib.pyplot as plt

# رسم منحنى خسارة التدريب
plt.plot(history_lstm.history['loss'], label='LSTM Loss')
plt.plot(history_lstm.history['val_loss'], label='LSTM Val Loss')
plt.plot(history_attention.history['loss'], label='Attention Loss')
plt.plot(history_attention.history['val_loss'], label='Attention Val Loss')
plt.legend()
plt.title("Loss Curves")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.show()

# رسم منحنى الدقة
plt.plot(history_lstm.history['accuracy'], label='LSTM Accuracy')
plt.plot(history_lstm.history['val_accuracy'], label='LSTM Val Accuracy')
plt.plot(history_attention.history['accuracy'], label='Attention Accuracy')
plt.plot(history_attention.history['val_accuracy'], label='Attention Val Accuracy')
plt.legend()
plt.title("Accuracy Curves")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.show()





















